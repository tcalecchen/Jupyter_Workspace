{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主題 01-3. 作業: 改造你的 1 號神經網路\n",
    "\n",
    "本週的作業很簡單, 就是把上課手寫辨識的範例改造一下, 看能不能讓成效變得更好。因為是第一次作業, 我們不特別要求要達到什麼水準, 只要「測試資料」的正確率還在 9 成以上就可以。我們這美好的 MNIST 例子, 你基本上亂亂作都是可以達成的, 不用擔心!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 初始準備\n",
    "\n",
    "這裡是一樣的我們不再說明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND=tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再來是我們標準數據分析動作!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 讀入 MNIST 數據庫"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "這裡也是一樣的! 好消息是如果你之前跟著做過, 現在會快很多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 改造你的神經網路\n",
    "\n",
    "### 3.1 改變結構\n",
    "\n",
    "我們本來用了兩層的隱藏層, 你可以改不同層數試試。比方說, 你可以試真的「深度學習」, 也就是隱藏層加到 3 層或以上。不過你需要有電腦彷如當機的心理準備...\n",
    "\n",
    "另外也可改變神經元的數目。比如說本來是這樣設的一個隱藏層, 我們用了 500 個神經元是這樣寫的。\n",
    "\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "因此如果要改 520 個神經元, 就會像這樣:\n",
    "\n",
    "    model.add(Dense(520))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "是不是很簡單?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 換個 activation function!\n",
    "\n",
    "在 Keras 中要換個 activation function 很簡單, 比如說我們要換當巨星 ReLU, 就在每層設 activation function 時說就好了。\n",
    "\n",
    "    model.add(Dense(500))\n",
    "    model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalize 我們的數據\n",
    "\n",
    "我們的每筆輸入是 784 維的\n",
    "\n",
    "$$\\mathbf{x} = (x_1, x_2, \\ldots, x_{784})$$\n",
    "\n",
    "我們會說這個神經網路有 784 個 features。現在我們每一個 $x_i$ 都是 0 到 255 的整數, 我們常喜歡把每個 $x_i$ 壓縮到一個更小的範圍, 一般有兩種作法:\n",
    "\n",
    "1. 每個 $x_i$ 都壓到 [0,1] 中的一個數\n",
    "2. 讓每個 $x_i$ 由平均值是 0, 標準差是 1 的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們的例子來做做看!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以試試看就知道, 真的都壓到 0-1 之間的數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train[87]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "還是可以畫的哦! 指令和以前完全一樣!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14459e898>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADmpJREFUeJzt3X+MVfWZx/HPo9KgQ1EJoyUUnS6i\n8UdcakZchRiNQkRJsFG0RCtNKsMfnbiN/WMNIamJP6Lrli5/bKqDYBFbSiOykkjcGqK4jU3jgAZl\ncbeos4XlxwzBBCoh5cezf8yhmeLc7xnuPfeeO/O8X4m5957nnHseb/jMufd+zz1fc3cBiOesshsA\nUA7CDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqHMaubPx48d7W1tbI3cJhNLT06MDBw7YUNat\nKfxmdoekZZLOlvSiuz+TWr+trU3d3d217BJAQnt7+5DXrfptv5mdLenfJM2WdJWk+WZ2VbXPB6Cx\navnMP03STnf/zN3/IunXkuYW0xaAeqsl/BMl7RrweHe27G+YWYeZdZtZd19fXw27A1CkWsI/2JcK\nX/l9sLt3uXu7u7e3trbWsDsARaol/LslTRrw+JuS9tTWDoBGqSX870uaYmbfMrOvSfqupA3FtAWg\n3qoe6nP342bWKek/1D/Ut9LdtxfWGYC6qmmc3903StpYUC8AGojTe4GgCD8QFOEHgiL8QFCEHwiK\n8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8I\nivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiqpll6zaxH0mFJJyQdd/f2IpoCUH81hT9zq7sf\nKOB5ADQQb/uBoGoNv0v6rZltMbOOIhoC0Bi1vu2f7u57zOwiSW+Z2Sfu/u7AFbI/Ch2SdMkll9S4\nOwBFqenI7+57stteSeslTRtknS53b3f39tbW1lp2B6BAVYffzFrM7Oun7kuaJenjohoDUF+1vO2/\nWNJ6Mzv1PL9y9zcL6QpA3VUdfnf/TNLfF9gLgAZiqA8IivADQRF+ICjCDwRF+IGgCD8QVBG/6kMT\n6+vrS9bXr1+frD/77LPJ+ueff37GPZ3i7sl6dg5JRYsWLUrWlyxZUrE2ceLE5LYRcOQHgiL8QFCE\nHwiK8ANBEX4gKMIPBEX4gaAY5x8G9u3bl6x/8MEHFWtz585NbnvixImqejolbyy+XttKUldXV7K+\nYsWKirXOzs7ktkuXLq2qp+GEIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBMU4fxN4++23k/U5c+Yk\n66nfxdc6jj979uxkffv27cn6k08+WbF20003Jbe97LLLkvU8qf/3NWvWJLdlnB/AiEX4gaAIPxAU\n4QeCIvxAUIQfCIrwA0HljvOb2UpJcyT1uvs12bJxktZKapPUI+k+d/+ifm2ObJs2bUrWjx49WvVz\nP/DAA8n6Cy+8kKyfc076n0jeeQSjR4+uWDt58mRy208//TRZnzx5crKecu+991a97UgxlCP/LyTd\ncdqyxyRtcvcpkjZljwEMI7nhd/d3JR08bfFcSauy+6sk3V1wXwDqrNrP/Be7+15Jym4vKq4lAI1Q\n9y/8zKzDzLrNrDtv3jgAjVNt+Peb2QRJym57K63o7l3u3u7u7a2trVXuDkDRqg3/BkkLsvsLJL1e\nTDsAGiU3/Ga2RtLvJV1hZrvN7AeSnpE008z+KGlm9hjAMJI7zu/u8yuUbiu4lxGrt7fipyJJ0vPP\nP1+3fc+YMSNZP/fcc2t6/lGjRiXrR44cqVhbsGBBxZokvffee1X1NBTTpk2r23MPF5zhBwRF+IGg\nCD8QFOEHgiL8QFCEHwiKS3c3wK233pqsf/FF+tfQV1xxRbJ+++23V6zNmzcvue3mzZuT9euvvz5Z\nP3DgQLJ+8803V6zt2rUruW2esWPHJutvvvlmxdrUqVNr2vdIwJEfCIrwA0ERfiAowg8ERfiBoAg/\nEBThB4JinL8AeWPde/bsSdY7OjqS9eeeey5ZP+usyn/D8y69ff755yfrX375ZbJ+7NixZL3WsfyU\n5cuXJ+s33HBD3fY9EnDkB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOcvQN4U24cOHUrWX3vttWT9\nkUceSdavvPLKZD2l1t+1b9mypabtUy6//PJkfc6cOXXbdwQc+YGgCD8QFOEHgiL8QFCEHwiK8ANB\nEX4gqNxxfjNbKWmOpF53vyZb9rikhZL6stUWu/vGejXZ7PLGmy+44IJkPe96AHnX/b/66qsr1u66\n666qt5WkV199NVlft25dsl6LJ554IlkfPXp03fYdwVCO/L+QdMcgy3/m7lOz/8IGHxiucsPv7u9K\nOtiAXgA0UC2f+TvNbJuZrTSzCwvrCEBDVBv+n0uaLGmqpL2SflppRTPrMLNuM+vu6+urtBqABqsq\n/O6+391PuPtJScslTUus2+Xu7e7e3traWm2fAApWVfjNbMKAh9+R9HEx7QBolKEM9a2RdIuk8Wa2\nW9JPJN1iZlMluaQeSYvq2COAOsgNv7vPH2Txijr0Mmy1tLQk61u3bk3WFy5cmKznXS/gnXfeqapW\ntrFjxybr1157bYM6iYkz/ICgCD8QFOEHgiL8QFCEHwiK8ANBcenuBrj00kuT9bxLd7/xxhvJ+sMP\nP1yxduTIkeS248aNS9YfffTRZH3JkiXJekpXV1eynnfpbtSGIz8QFOEHgiL8QFCEHwiK8ANBEX4g\nKMIPBMU4fxMYM2ZMsn7//fcn69ddd13F2tGjR5Pb5l1WfNmyZcl6nnvuuadi7c4776zpuVEbjvxA\nUIQfCIrwA0ERfiAowg8ERfiBoAg/EBTj/CPAlClTKtbyxvmffvrpZH316tXJeuocA0l66aWXKtby\nLnmO+uLIDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANB5Y7zm9kkSS9L+oakk5K63H2ZmY2TtFZSm6Qe\nSfe5+xf1axXV2Lx5c7L+1FNPJet5Y/F55wkwlt+8hnLkPy7px+5+paR/kPRDM7tK0mOSNrn7FEmb\nsscAhonc8Lv7Xnffmt0/LGmHpImS5kpala22StLd9WoSQPHO6DO/mbVJ+rakP0i62N33Sv1/ICRd\nVHRzAOpnyOE3szGS1kn6kbsfOoPtOsys28y6+/r6qukRQB0MKfxmNkr9wf+lu5+aVXK/mU3I6hMk\n9Q62rbt3uXu7u7e3trYW0TOAAuSG38xM0gpJO9x96YDSBkkLsvsLJL1efHsA6mUoP+mdLul7kj4y\nsw+zZYslPSPpN2b2A0l/kjSvPi0iz7Zt2yrWHnzwwZqe+5VXXknWZ86cWdPzozy54Xf330myCuXb\nim0HQKNwhh8QFOEHgiL8QFCEHwiK8ANBEX4gKC7dPQwcO3YsWZ8/f37F2sGDB5Pb5k2TfdttjOaO\nVBz5gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAoxvmbwPHjx5P1zs7OZP2TTz6pWLvxxhuT265duzZZ\nP++885J1DF8c+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMb5m8DGjRuT9RdffDFZnzVrVsVa3hTa\njOPHxZEfCIrwA0ERfiAowg8ERfiBoAg/EBThB4LKHec3s0mSXpb0DUknJXW5+zIze1zSQkl92aqL\n3T09YB3Uzp07k/WHHnqopudfvXp1xdr48eNrem6MXEM5yee4pB+7+1Yz+7qkLWb2Vlb7mbv/S/3a\nA1AvueF3972S9mb3D5vZDkkT690YgPo6o8/8ZtYm6duS/pAt6jSzbWa20swurLBNh5l1m1l3X1/f\nYKsAKMGQw29mYyStk/Qjdz8k6eeSJkuaqv53Bj8dbDt373L3dndvb21tLaBlAEUYUvjNbJT6g/9L\nd39Nktx9v7ufcPeTkpZLmla/NgEULTf8ZmaSVkja4e5LByyfMGC170j6uPj2ANTLUL7tny7pe5I+\nMrMPs2WLJc03s6mSXFKPpEV16XAEmDFjRrJ++PDhZH369OnJektLyxn3BAzl2/7fSbJBSozpA8MY\nZ/gBQRF+ICjCDwRF+IGgCD8QFOEHguLS3Q2wb9++slsAvoIjPxAU4QeCIvxAUIQfCIrwA0ERfiAo\nwg8EZe7euJ2Z9Un63wGLxks60LAGzkyz9tasfUn0Vq0ie7vU3Yd0vbyGhv8rOzfrdvf20hpIaNbe\nmrUvid6qVVZvvO0HgiL8QFBlh7+r5P2nNGtvzdqXRG/VKqW3Uj/zAyhP2Ud+ACUpJfxmdoeZ/beZ\n7TSzx8rooRIz6zGzj8zsQzPrLrmXlWbWa2YfD1g2zszeMrM/ZreDTpNWUm+Pm9n/Za/dh2Z2Z0m9\nTTKzt81sh5ltN7N/zJaX+tol+irldWv4234zO1vS/0iaKWm3pPclzXf3/2poIxWYWY+kdncvfUzY\nzG6W9GdJL7v7Ndmyf5Z00N2fyf5wXuju/9QkvT0u6c9lz9ycTSgzYeDM0pLulvR9lfjaJfq6TyW8\nbmUc+adJ2unun7n7XyT9WtLcEvpoeu7+rqSDpy2eK2lVdn+V+v/xNFyF3pqCu+91963Z/cOSTs0s\nXeprl+irFGWEf6KkXQMe71ZzTfntkn5rZlvMrKPsZgZxcTZt+qnp0y8quZ/T5c7c3EinzSzdNK9d\nNTNeF62M8A82+08zDTlMd/frJM2W9MPs7S2GZkgzNzfKIDNLN4VqZ7wuWhnh3y1p0oDH35S0p4Q+\nBuXue7LbXknr1XyzD+8/NUlqdttbcj9/1UwzNw82s7Sa4LVrphmvywj/+5KmmNm3zOxrkr4raUMJ\nfXyFmbVkX8TIzFokzVLzzT68QdKC7P4CSa+X2MvfaJaZmyvNLK2SX7tmm/G6lJN8sqGMf5V0tqSV\n7v5Uw5sYhJn9nfqP9lL/lY1/VWZvZrZG0i3q/9XXfkk/kfTvkn4j6RJJf5I0z90b/sVbhd5uUf9b\n17/O3HzqM3aDe5sh6T8lfSTpZLZ4sfo/X5f22iX6mq8SXjfO8AOC4gw/ICjCDwRF+IGgCD8QFOEH\ngiL8QFCEHwiK8ANB/T9rpwaaaWt1iwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f7bdba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[87], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把輸入的資料做 normalization 是很好的習慣。原因有好幾個, 比方說:\n",
    "\n",
    "1. 每個 feature 範圍不同時, 常常會由數值大的主導。Normalization 可以避免這種事情發生。\n",
    "2. 因為像 ReLU 在大於零的部份就是 $f(x) = x$ 這樣的函數, $x$ 越大, 值就越大, 甚至非常大。\n",
    "3. 再來是穩定性的問題, 神經網路的學理研究就喜歡把所有 feature, 所有 weight 都假設在「平均值 0, 標準差 1」的常態分布。這樣的狀況也證實有各種良好特質。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 優化方式的調整\n",
    "\n",
    "### 5.1 改善學習方式的兩大考量\n",
    "\n",
    "我們用 SGD 可以說是最標準的方式, 事實上所有優化 (學習) 方式基本上都是 gradient descent。不過我們有兩大方向可以考量改善:\n",
    "\n",
    "#### (1) 加入 momentum\n",
    "\n",
    "想像我們的 loss function 畫出來像一座高低起伏的山, 我們在山中的某一個點, 想要走到山下。Gradient descent 會帶我們從「目前看到下坡最陡的方向」下山。但是走了兩步後我們可能發現 gradient descent 又說要走另一個方向。於是在那繞來繞去才終於下山。因此我們想要以更「穩定而勇敢」的方式, 比較朝向一個目標前進, 就加入所謂的 momentum。\n",
    "\n",
    "#### (2) learning rate 變速器\n",
    "\n",
    "我們說 learning rate 調夠小, 我們的神經網路基本上都該收斂的。不過調太小可能學非常慢, 大了一點又怕跳過頭。於是有一些「變速」的方式被研發出來, 一般的原則就是「開始快、接近目標時變慢」。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 在 Keras 選用不同學習法!\n",
    "\n",
    "\n",
    "我們就先 import 想要的學習法, 如果不知道可以考慮用 Adam, 前兩大項考量 Adam 都有考慮進去!\n",
    "\n",
    "    from keras.optimizers import Adam\n",
    "\n",
    "然後在 compile 時說要用 Adam 即可。\n",
    "\n",
    "    model.compile(loss='mse', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "Adam 其實有些參數可調, 但我們這暫不多談。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
